{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgaw+J7rWPO6U24dHV03Lx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Terrific Translation with Transformers\n",
        "\n",
        "Transformers were developed in the natural language processing (NLP) domain and now form a key component of the cutting edge deep learning architectures, including the recent architecture [Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion).\n",
        "\n",
        "In this demo, we will focus on the NLP domain and build a transformer from scratch to translate German to English. Our code is based on the excellent pytorch tutorial [here](https://pytorch.org/tutorials/beginner/translation_transformer.html)."
      ],
      "metadata": {
        "id": "VvYQfJ72Bln-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries! Bibliotheken!\n",
        "\n",
        "We start by loading the (numerous) libraries we will use."
      ],
      "metadata": {
        "id": "ah5BCnVYDk3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All libraries\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from timeit import default_timer as timer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "U6stAzq8BlId"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Set Up\n",
        "\n",
        "In order to work with language, we need to convert words as we read them in letters to \"tokens\" which are a numeric representation. This process is called tokenization and we will install both the dataset and tokenizers for English and German on our machine. (The following code should only be run once and should take about a minute to run.)"
      ],
      "metadata": {
        "id": "r06ydV4BCKen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_run = False\n",
        "# Run once to install dataset and tokenizers for English and German\n",
        "if first_run:\n",
        "  !pip install -U torchdata\n",
        "  !pip install -U spacy\n",
        "  !python -m spacy download en_core_web_sm\n",
        "  !python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "id": "P0NUQRXRCQiz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will load the datasets and store the tokenizers."
      ],
      "metadata": {
        "id": "UBjtWZLeCstO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "  language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "  for data_sample in data_iter:\n",
        "    yield token_transform[language](data_sample[language_index[language]])"
      ],
      "metadata": {
        "id": "zvBUhoJCBsrg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are special characters which will use to structure the sequences of tokens (representing sentences of words). The `<box>` token starts a sequence while the `<eos>` token ends it. `<pad>` is used for sequences of length less than the maximum and `<unk>` is used for unknown words."
      ],
      "metadata": {
        "id": "X0n9JCGdJXFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0,1,2,3\n",
        "special_symbols = ['<unk>', '<pad>', '<box>', '<eos>']\n",
        "\n",
        "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "  vocab_transform[language] = build_vocab_from_iterator(yield_tokens(train_iter, language),\n",
        "                                                        specials=special_symbols)\n",
        "                                                        \n",
        "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[language].set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "id": "gJMmjqnSJWlH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq Network\n",
        "\n",
        "We will use the Seq2Seq model introduced in \"[Attention Is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\".\n",
        "\n",
        "The first step of the model is to embed the tokenized words into a smaller latent space that captures the meaning of the words."
      ],
      "metadata": {
        "id": "rtldDo01Dl_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
      ],
      "metadata": {
        "id": "JKLGbIZcEo8n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike recurrent networks, self-attention does not automatically capture the order that words appear. We need to fix this with a special positional encoding. Naive positional encodings like a one-hot approach or normalizing the index between $[0,1]$ fail due to variable sequence lenghts. The very clever solution in the original paper is to use a vector of sinusoidal functions at different frequencies. The video linked [here](https://www.youtube.com/watch?v=dichIcUZfOw) provides a high-level introduction while the article [here](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/) explains more of the math behind why they work. In particular, the positional encoding is able to capture the *relative* distance between indices."
      ],
      "metadata": {
        "id": "fLKQDYyhEpOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
      ],
      "metadata": {
        "id": "7cza0HMZD3hH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the tools to embed the tokens, we will define the transformer architecture. Notice that all the exciting self-attention gets hidden under the hood in `Transformer`. We will peel this back a little at the end when we investigate the weights of the self-attention mechanism."
      ],
      "metadata": {
        "id": "rzk7-HLIKvNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int, emb_size: int,\n",
        "                 nhead: int, src_vocab_size: int, tgt_vocab_size: int, dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size, nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor, tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor, tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "metadata": {
        "id": "B61YzMRyFIdA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A key part of training is hiding the tokens we haven't seen yet (otherwise, the problem would be very easy!)."
      ],
      "metadata": {
        "id": "M9Olx53NK-46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "OauEompFGQJh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization\n",
        "\n",
        "We initialize the model with the standard hyperparameters chosen in the original paper. The resulting number of parameters is huge!!"
      ],
      "metadata": {
        "id": "7ru8zZsoDQsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n"
      ],
      "metadata": {
        "id": "gPmTO9uIHm-O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the Xavier initialization (Gaussian with a special variance) in order to keep the norm of the weights close to 1."
      ],
      "metadata": {
        "id": "HIMEtKIgLZm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for p in transformer.parameters():\n",
        "    count += p.numel()\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "# Architecture\n",
        "transformer = transformer.to(DEVICE)\n",
        "# Loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "# Optimization method\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "print(f'There are {count} parameters in the model!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDCljThSLYul",
        "outputId": "34851cbb-61fa-4966-a649-e0d4b4107dd2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 33570389 parameters in the model!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also need some helper functions for processing the inputs."
      ],
      "metadata": {
        "id": "JR1rAgl-LmbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tesors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ],
      "metadata": {
        "id": "evkcDw5hKCGO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Finally, we will get to train the model!"
      ],
      "metadata": {
        "id": "JZSoS8JiDWoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(model, runtype: str, optimizer=None):\n",
        "  assert runtype in ['train', 'valid']\n",
        "  if runtype == 'train': model.train()\n",
        "  else: model.eval()\n",
        "  \n",
        "  losses = 0\n",
        "  data_iter = Multi30k(split=runtype, language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "  dataloader = DataLoader(data_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "  num_points = 0\n",
        "\n",
        "  for src, tgt in dataloader:\n",
        "    num_points += src.shape[1]\n",
        "    src = src.to(DEVICE)\n",
        "    tgt = tgt.to(DEVICE)\n",
        "\n",
        "    tgt_input = tgt[:-1, :]\n",
        "\n",
        "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "    logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask,\n",
        "                   tgt_padding_mask, src_padding_mask)\n",
        "    \n",
        "    if runtype == 'train': optimizer.zero_grad()\n",
        "\n",
        "    tgt_out = tgt[1:, :]\n",
        "    loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "\n",
        "    if runtype == 'train':\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "    losses += loss.item()\n",
        "  \n",
        "  return losses / num_points\n"
      ],
      "metadata": {
        "id": "NRDrvU87Kj2o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the model has more than 30 million parameters, we'll be able to train it for 10 epochs (in about 6 minutes on colab GPU). While not perfect, the results are pretty good!"
      ],
      "metadata": {
        "id": "4iFRGdXwL48C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  start_time = timer()\n",
        "  train_loss = run(transformer, runtype='train', optimizer=optimizer)\n",
        "  end_time = timer()\n",
        "  val_loss = run(transformer, runtype='valid')\n",
        "  print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEEUl_NFOsMJ",
        "outputId": "e8fb2114-d3a7-4095-83be-864493270ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train loss: 0.042, Val loss: 0.032, Epoch time = 40.146s\n",
            "Epoch: 1, Train loss: 0.029, Val loss: 0.026, Epoch time = 39.424s\n",
            "Epoch: 2, Train loss: 0.025, Val loss: 0.023, Epoch time = 38.848s\n",
            "Epoch: 3, Train loss: 0.022, Val loss: 0.021, Epoch time = 38.938s\n",
            "Epoch: 4, Train loss: 0.019, Val loss: 0.019, Epoch time = 39.020s\n",
            "Epoch: 5, Train loss: 0.018, Val loss: 0.018, Epoch time = 39.290s\n",
            "Epoch: 6, Train loss: 0.016, Val loss: 0.017, Epoch time = 39.032s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actual Translation\n",
        "\n",
        "We've trained the weights to give a good estimate of what German sentences correspond to in English. In order to do the translation, we'll need to pick the English words with the highest probability given by the model."
      ],
      "metadata": {
        "id": "4OLSrlp4DZwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "  src = src.to(DEVICE)\n",
        "  src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "  memory = model.encode(src, src_mask)\n",
        "  ys = torch.ones(1,1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "  for i in range(max_len-1):\n",
        "    memory = memory.to(DEVICE)\n",
        "    tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
        "    out = model.decode(ys, memory, tgt_mask)\n",
        "    out = out.transpose(0,1)\n",
        "    prob = model.generator(out[:,-1])\n",
        "    _, next_word = torch.max(prob, dim=1)\n",
        "    next_word = next_word.item()\n",
        "    ys = torch.cat([ys, torch.ones(1,1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "    if next_word == EOS_IDX:\n",
        "      break\n",
        "  return ys\n",
        "\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<box>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "id": "_YVpu8aMPsj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's translate some German into English. For fun, we can also run this code *before* and *after* we train. Amazingly, the model works pretty well! "
      ],
      "metadata": {
        "id": "u-k1TqwXMX5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    'Eine Gruppe von Menschen steht vor einem Iglu .',\n",
        "]\n",
        "for sentence in sentences:\n",
        "  print(translate(transformer, sentence))"
      ],
      "metadata": {
        "id": "xNG950FIUm-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Investigating the Self-Attention Weights\n",
        "\n",
        "Remember that the key component of this model is self-attention. Let's take a specia look at it. First, we'll need to get some validation data to investigate with."
      ],
      "metadata": {
        "id": "_hOiwpsmDcrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "dataloader = DataLoader(data_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "source, target = next(iter(dataloader))\n",
        "source = source.to(DEVICE)\n",
        "target = target.to(DEVICE)"
      ],
      "metadata": {
        "id": "7haKKx5JOagJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to run the model on the data, we'll need to get masks and embedding. Then we'll add a positional encoding and feed these through the first self-attention head in the transformer."
      ],
      "metadata": {
        "id": "KqiUAxdwMzSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(source, target)\n",
        "src_emb = transformer.positional_encoding(transformer.src_tok_emb(source))\n",
        "encoderlayer = transformer.transformer.encoder.layers[0]\n",
        "x = encoderlayer.norm1(src_emb)\n",
        "output, weights = encoderlayer.self_attn(x, x, x, attn_mask = src_mask, key_padding_mask=src_padding_mask)"
      ],
      "metadata": {
        "id": "HSXuW5nWPpOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's look at an example sentence and visualize the similarity that the model sees between embedded words."
      ],
      "metadata": {
        "id": "zGFEWRQgNPxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "src_sentence = vocab_transform[SRC_LANGUAGE].lookup_tokens(list(source[:,index]))\n",
        "tgt_sentence = vocab_transform[TGT_LANGUAGE].lookup_tokens(list(target[:,index]))\n",
        "end_index = src_sentence.index('<eos>')\n",
        "print(' '.join(src_sentence).replace('<box>', '').replace('<pad>', '').replace('<eos>', ''))\n",
        "print(' '.join(tgt_sentence).replace('<box>', '').replace('<pad>', '').replace('<eos>', ''))"
      ],
      "metadata": {
        "id": "jBVHFyL8OnLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(weights[index].detach().numpy()[:end_index, :end_index])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FmA6kyT4-JTj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}